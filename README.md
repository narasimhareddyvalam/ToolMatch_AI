# âš¡ ToolMatch AI

**ToolMatch AI** is a fine-tuned **T5-small** model that translates vague business automation requests into **structured low-code workflows**.  
It suggests the right combination of tools (like Airtable, Zapier, Slack, Gmail, Typeform) with clear **Trigger â†’ Action** logic â€” making automation accessible to startups, ops managers, and non-technical teams:contentReference[oaicite:0]{index=0}.

---

## âœ¨ Project Summary
- **Model:** Hugging Face `t5-small`, fine-tuned with 250 curated examples  
- **Goal:** Convert plain English tasks (e.g., â€œRemind customers about overdue invoicesâ€) into structured automation recipes  
- **Training:** Hugging Face Transformers, Seq2SeqTrainer, Google Colab (GPU)  
- **UI:** Gradio web interface for interactive testing  
- **Output Example:**  
Use Airtable + Zapier + Gmail.
Trigger: When invoice is overdue in Airtable â†’ Action: Send email via Gmail.

---

## ğŸ“‚ Dataset Preparation
- **Format:** 250 samples in `.jsonl` format (`{"input": "...", "output": "..."}`)  
- **Steps Taken:**
- Deduplication of prompts  
- Normalization of phrasing  
- Verified `Trigger â†’ Action` structure  
- Added 100 diverse examples across invoicing, CRM, onboarding, HR:contentReference[oaicite:1]{index=1}  

ğŸ“Š **Tool frequency:**  
- Zapier (26.7%)  
- Airtable (20%)  
- Gmail/Slack/Typeform/Mailchimp (~6.7% each)

---

## ğŸ§  Model Selection
- **Why T5-small?**
- Text-to-text framing aligns perfectly with workflow generation  
- Lightweight & efficient for free Colab GPUs  
- Encoder-decoder structure ensures controlled outputs:contentReference[oaicite:2]{index=2}  

- **Alternatives considered:**  
- GPT-2 (discarded: decoder-only, less structured)  
- BART (discarded: larger & slower for Colab)  
- FLAN-T5 (tested, limited benefit for Trigger â†’ Action formatting)

---

## âš™ï¸ Fine-Tuning Setup
- **Framework:** Hugging Face `Seq2SeqTrainer`  
- **Configuration:**  
- Learning Rate: **5e-4** (best after sweep)  
- Batch Size: 8  
- Epochs: 5  
- **Result:** Final training loss â‰ˆ **0.0644** with stable convergence:contentReference[oaicite:3]{index=3}  

---

## ğŸ“Š Model Evaluation
- Compared learning rates (2e-4, 3e-4, 5e-4)  
- **Best result:** LR = 5e-4 â†’ lowest loss (0.0644)  
- âœ… Effective generalization to structured business tasks:contentReference[oaicite:4]{index=4}

---

## ğŸ” Error Analysis
Tested on tricky prompts:  
- *â€œBuild a dashboard without writing codeâ€* â†’ Partial relevance, missed core idea  
- *â€œPredict which customers are unhappyâ€* â†’ Misinterpreted predictive nature  
- *â€œHelp team feel more connectedâ€* â†’ Misaligned, treated as customer service task  

ğŸ“‰ **Insight:** ToolMatch struggles with **abstract or emotional prompts**, highlighting the need for broader training:contentReference[oaicite:5]{index=5}.

---

## ğŸ–¥ï¸ Inference Pipeline
Two modes of interaction:  
1. **Console function** (`generate_tool_suggestion()`) for direct testing  
2. **Gradio Web App**  
 - Textbox input for tasks  
 - Predict button â†’ structured tool recipe  
 - Dark theme, responsive layout, emojis for clarity:contentReference[oaicite:6]{index=6}  

---

## ğŸ“¦ Repository Structure
toolmatch-ai/
â”œâ”€ ToolMatchAI.pdf # Full project report
â”œâ”€ demo.mp4 # Demo video (optional, add via Git LFS if >100MB)
â”œâ”€ dataset/ # Training dataset (.jsonl)
â”œâ”€ notebook/ # Colab/Notebooks for fine-tuning & inference
â”œâ”€ model/ # Saved fine-tuned model + tokenizer
â”œâ”€ app/ # Gradio web app files
â”œâ”€ README.md # You are here

---

## ğŸš§ Limitations
- âŒ Struggles with vague/multi-intent prompts  
- âŒ Limited generalization (only 250 samples)  
- âŒ No safety filters for risky automations  
- âŒ Not yet deployed as a hosted public app:contentReference[oaicite:7]{index=7}  

---

## ğŸ™Œ Acknowledgments
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)  
- [Gradio](https://www.gradio.app/guides)  
- [Google Colab](https://colab.research.google.com/) for free GPU fine-tuning  
